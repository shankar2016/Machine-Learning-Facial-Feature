{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Classification with Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interest in neural networks, and in particular those with architechures that support deep learning, has been surging in recent years.  \n",
    "\n",
    "In this notebook we will be revisiting the problem of digit classification on the MNIST data.  In doing so, we will introduce a new Python library, Theano, for working with neural networks.  Theano is a popular choice for neural networks as the same code can be run on either CPUs or GPUs.  GPUs greatly speed up the training and prediction, and is readily available. Amazon even offers GPU machines on EC2.  \n",
    "\n",
    "In part 1, we'll introduce Theano, and refresh ourselves on the MNIST dataset.  In part 2, we'll create a multi-layer neural network with a simple architechure, and train it using backpropagation.  Part 3 will introduce the convolutional architechure, which can be said to be doing 'deep learning' (also called feature learning or representation learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start to look at Theano.  If later you'd like to go deeper into Theano, you may want to read this paper: http://www.iro.umontreal.ca/~lisa/pointeurs/theano_scipy2010.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Theano if you haven't already.  Then let's load it, and set it to work with a CPU.  For reference, here is the Theano documentation: http://www.deeplearning.net/software/theano/library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time\n",
    "\n",
    "import theano \n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "print (theano.config.device) # We're using CPUs (for now)\n",
    "print (theano.config.floatX) # Should be 64 bit for CPUs\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for MNIST data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features = 784\n",
      "Train set = 2000\n",
      "Test set = 2000\n"
     ]
    }
   ],
   "source": [
    "# Repeating steps from Project 1 to prepare mnist dataset. \n",
    "mnist = fetch_mldata('MNIST original', data_home='~/datasets/mnist')\n",
    "X, Y = mnist.data, mnist.target\n",
    "X = X / 255.0\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, Y = X[shuffle], Y[shuffle]\n",
    "numExamples = 2000\n",
    "test_data, test_labels = X[70000-numExamples:], Y[70000-numExamples:]\n",
    "train_data, train_labels = X[:numExamples], Y[:numExamples]\n",
    "numFeatures = train_data[1].size\n",
    "numTrainExamples = train_data.shape[0]\n",
    "numTestExamples = test_data.shape[0]\n",
    "print ('Features = %d' %(numFeatures))\n",
    "print ('Train set = %d' %(numTrainExamples))\n",
    "print ('Test set = %d' %(numTestExamples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking ahead to working with neural networks, let's prepare one additional variation of the label data.  Let's make these labels, rather than each being an integer value from 0-9, be a set of 10 binary values, one for each class.  This is sometimes called a 1-of-n encoding, and it makes working with Neural Networks easier, as there will be one output node for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "binarized_data = np.zeros((train_labels.size,10))\n",
    "print(binarized_data.size)\n",
    "data = train_labels\n",
    "for j in range(0,int(data.size/100)):\n",
    "    feature = data[j:j+1]\n",
    "    i = feature.astype(np.int64) \n",
    "    binarized_data[j,i]=1\n",
    "    print(binarized_data[j,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes = 10\n"
     ]
    }
   ],
   "source": [
    "def binarizeY(data):\n",
    "    binarized_data = np.zeros((data.size,10))\n",
    "    for j in range(0,data.size):\n",
    "        feature = data[j:j+1]\n",
    "        i = feature.astype(np.int64) \n",
    "        binarized_data[j,i]=1\n",
    "    return binarized_data\n",
    "train_labels_b = binarizeY(train_labels)\n",
    "test_labels_b = binarizeY(test_labels)\n",
    "numClasses = train_labels_b[1].size\n",
    "print ('Classes = %d' %(numClasses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with a KNN model to establish a baseline accuracy.\n",
    "\n",
    "Exercise: You've seen a number of different classification algorithms (e.g. naive bayes, decision trees, random forests, logistic regression) at this point.  How does KNN scalability and accuracy with respect to the size of the training dataset compare to those other algorithms?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time = 0.13\n",
      "Accuracy = 0.9130\n",
      "Prediction time = 7.04\n"
     ]
    }
   ],
   "source": [
    "neighbors = 1\n",
    "knn = KNeighborsClassifier(neighbors)\n",
    "# we'll be waiting quite a while if we use 60K examples, so let's cut it down.  You may want to run the full 60K on your own later to see what the accuracy is.\n",
    "mini_train_data, mini_train_labels = X[:numExamples], Y[:numExamples] \n",
    "start_time = time.time()\n",
    "knn.fit(mini_train_data, mini_train_labels)\n",
    "print ('Train time = %.2f' %(time.time() - start_time))\n",
    "start_time = time.time()\n",
    "accuracy = knn.score(test_data, test_labels)\n",
    "print ('Accuracy = %.4f' %(accuracy))\n",
    "print ('Prediction time = %.2f' %(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now that we have a simple baseline, let's start working in Theano.  Before we jump to multi-layer neural networks though, let's train a logistic regression model to make certain we're using Theano correctly. \n",
    "\n",
    "Recall from Josh's regression lecture the four key components: (1) parameters, (2) model, (3) cost function, and (4) objective. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(numFeatures)\n",
    "print(numClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## (1) Parameters \n",
    "# Initialize the weights to small, but non-zero, values.\n",
    "w = theano.shared(np.asarray((np.random.randn(*(numFeatures, numClasses))*.01)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two notes relevant at this point:\n",
    "\n",
    "First, logistic regression can be thought of as a neural network with no hidden layers. The output values are just the dot product of the inputs and the edge weights.\n",
    "\n",
    "Second, we have 10 classes. We can either train separate one vs all classifiers using sigmoid activation, which would be a hassle, or we can use the softmax activation, which is essentially a multi-class version of sigmoid. We'll use Theano's built-in implementation of softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## (2) Model\n",
    "# Theano objects accessed with standard Python variables\n",
    "X = T.matrix()\n",
    "Y = T.matrix()\n",
    "\n",
    "def model(X, w):\n",
    "    return T.nnet.softmax(T.dot(X, w))\n",
    "y_hat = model(X, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use cross-entropy as a cost function.  Cross entropy only considers the error between the true class and the prediction, and not the errors for the false classes.  This tends to cause the network to converge faster.  We'll use Theano's built-in cross entropy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## (3) Cost function\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(y_hat, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is minimize the cost, and to do that we'll use batch gradient descent.\n",
    "\n",
    "Exercise: What are the differences between batch, stochastic, and mini-batch gradient descent?  What are the implications of each for working on large datasets?\n",
    "\n",
    "We'll use Theano's built-in gradient function.  Exercise: Do you recall from Josh's lecture what the gradient is for beta in logistic regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## (4) Objective (and solver)\n",
    "\n",
    "alpha = 0.01\n",
    "gradient = T.grad(cost=cost, wrt=w) \n",
    "update = [[w, w - gradient * alpha]] \n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True) # computes cost, then runs update\n",
    "y_pred = T.argmax(y_hat, axis=1) # select largest probability as prediction\n",
    "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
    "\n",
    "def gradientDescent(epochs):\n",
    "    trainTime = 0.0\n",
    "    predictTime = 0.0\n",
    "    for i in range(epochs):\n",
    "        start_time = time.time()\n",
    "        cost = train(train_data[0:len(train_data)], train_labels_b[0:len(train_data)])\n",
    "        trainTime =  trainTime + (time.time() - start_time)\n",
    "        print ('%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(test_labels_b, axis=1) == predict(test_data))))\n",
    "    print ('train time = %.2f' %(trainTime))\n",
    "\n",
    "gradientDescent(50)\n",
    "\n",
    "start_time = time.time()\n",
    "predict(test_data)   \n",
    "print ('predict time = %.2f' %(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise:  What do you expect to happen if we convert batch gradient descent to stochastic gradient descent?  Why?\n",
    "\n",
    "Let's try it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## (1) Parameters\n",
    "w = theano.shared(np.asarray((np.random.randn(*(numFeatures, numClasses))*.01)))\n",
    "\n",
    "## (2) Model\n",
    "X = T.matrix()\n",
    "Y = T.matrix()\n",
    "def model(X, w):\n",
    "    return T.nnet.softmax(T.dot(X, w))\n",
    "y_hat = model(X, w)\n",
    "\n",
    "## (3) Cost\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(y_hat, Y))\n",
    "\n",
    "## (4) Objective\n",
    "alpha = 0.01\n",
    "gradient = T.grad(cost=cost, wrt=w)\n",
    "update = [[w, w - gradient * alpha]] \n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True) \n",
    "y_pred = T.argmax(y_hat, axis=1) \n",
    "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
    "\n",
    "miniBatchSize = 1 \n",
    "def gradientDescentStochastic(epochs):\n",
    "    trainTime = 0.0\n",
    "    predictTime = 0.0\n",
    "    start_time = time.time()\n",
    "    for i in range(epochs):       \n",
    "        for start, end in zip(range(0, len(train_data), miniBatchSize), range(miniBatchSize, len(train_data), miniBatchSize)):\n",
    "            cost = train(train_data[start:end], train_labels_b[start:end])\n",
    "        trainTime =  trainTime + (time.time() - start_time)\n",
    "        print ('%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(test_labels_b, axis=1) == predict(test_data))))     \n",
    "    print ('train time = %.2f' %(trainTime))\n",
    "    \n",
    "gradientDescentStochastic(50)\n",
    "\n",
    "start_time = time.time()\n",
    "predict(test_data)   \n",
    "print ('predict time = %.2f' %(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: What do you expect to happen if you switch the batch size to be great than 1 (i.e. mini-batch)?  Why?\n",
    "\n",
    "Try it for a few values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## (1) Parameters\n",
    "w = theano.shared(np.asarray((np.random.randn(*(numFeatures, numClasses))*.01)))\n",
    "\n",
    "## (2) Model\n",
    "X = T.matrix()\n",
    "Y = T.matrix()\n",
    "def model(X, w):\n",
    "    return T.nnet.softmax(T.dot(X, w))\n",
    "y_hat = model(X, w)\n",
    "\n",
    "## (3) Cost\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(y_hat, Y))\n",
    "\n",
    "## (4) Objective\n",
    "alpha = 0.01\n",
    "gradient = T.grad(cost=cost, wrt=w)\n",
    "update = [[w, w - gradient * alpha]] \n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True) \n",
    "y_pred = T.argmax(y_hat, axis=1) \n",
    "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
    "\n",
    "miniBatchSize = 10 \n",
    "def gradientDescentStochastic(epochs):\n",
    "    trainTime = 0.0\n",
    "    predictTime = 0.0\n",
    "    start_time = time.time()\n",
    "    for i in range(epochs):       \n",
    "        for start, end in zip(range(0, len(train_data), miniBatchSize), range(miniBatchSize, len(train_data), miniBatchSize)):\n",
    "            cost = train(train_data[start:end], train_labels_b[start:end])\n",
    "        trainTime =  trainTime + (time.time() - start_time)\n",
    "        print ('%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(test_labels_b, axis=1) == predict(test_data))))     \n",
    "    print ('train time = %.2f' %(trainTime))\n",
    "    \n",
    "gradientDescentStochastic(50)\n",
    "\n",
    "start_time = time.time()\n",
    "predict(test_data)   \n",
    "print ('predict time = %.2f' %(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART 2: Multi-layer Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we start to get further into Neural Networks, if you'd like to take time on your own for an in-depth introduction on the state of the art in the topic, check out this excellent 1-day tutorial from KDD2014:\n",
    "\n",
    "Part 1: http://videolectures.net/kdd2014_bengio_deep_learning/\n",
    "\n",
    "Part 2: http://videolectures.net/tcmm2014_taylor_deep_learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "Let's take our implementation of logistic regression (which recall is in fact a single layer neural network), and add a hidden layer, making it a two layer neural network.  Because we have a hidden layer, we will now train the model using backpropagation.\n",
    "\n",
    "Exercise: How do you expect this model to compare to KNN and logistic regression in terms of train time and accuracy?  Why?\n",
    "\n",
    "Let's try it out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## (1) Parameters\n",
    "numHiddenNodes = 600 \n",
    "w_1 = theano.shared(np.asarray((np.random.randn(*(numFeatures, numHiddenNodes))*.01)))\n",
    "w_2 = theano.shared(np.asarray((np.random.randn(*(numHiddenNodes, numClasses))*.01)))\n",
    "params = [w_1, w_2]\n",
    "\n",
    "\n",
    "## (2) Model\n",
    "X = T.matrix()\n",
    "Y = T.matrix()\n",
    "# Two notes:\n",
    "# First, feed forward is the composition of layers (dot product + activation function)\n",
    "# Second, activation on the hidden layer still uses sigmoid\n",
    "def model(X, w_1, w_2):\n",
    "    return T.nnet.softmax(T.dot(T.nnet.sigmoid(T.dot(X, w_1)), w_2))\n",
    "y_hat = model(X, w_1, w_2)\n",
    "\n",
    "\n",
    "## (3) Cost...same as logistic regression\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(y_hat, Y))\n",
    "\n",
    "\n",
    "## (4) Minimization.  Update rule changes to backpropagation.\n",
    "alpha = 0.01\n",
    "def backprop(cost, w):\n",
    "    grads = T.grad(cost=cost, wrt=w)\n",
    "    updates = []\n",
    "    for w1, grad in zip(w, grads):\n",
    "        updates.append([w1, w1 - grad * alpha])\n",
    "    return updates\n",
    "update = backprop(cost, params)\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True)\n",
    "y_pred = T.argmax(y_hat, axis=1)\n",
    "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
    "\n",
    "miniBatchSize = 1\n",
    "def gradientDescentStochastic(epochs):\n",
    "    trainTime = 0.0\n",
    "    predictTime = 0.0\n",
    "    start_time = time.time()\n",
    "    for i in range(epochs):\n",
    "        for start, end in zip(range(0, len(train_data), miniBatchSize), range(miniBatchSize, len(train_data), miniBatchSize)):\n",
    "            cost = train(train_data[start:end], train_labels_b[start:end])\n",
    "        trainTime =  trainTime + (time.time() - start_time)\n",
    "        print ('%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(test_labels_b, axis=1) == predict(test_data))))\n",
    "    print ('train time = %.2f' %(trainTime))\n",
    "\n",
    "gradientDescentStochastic(50)\n",
    "\n",
    "start_time = time.time()\n",
    "predict(test_data)   \n",
    "print ('predict time = %.2f' %(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Change the number of nodes in the hidden layer?  What do you expect the impact to be?  What is the impact?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "As interest in networks with more layers and more complicated architechures has increased, a couple of tricks have emerged and become standard practice.  Let's look at two of those--rectifier activation and dropout noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise:  We saw an improvement from adding a hidden layer.  What do you expect to happen if a second hidden layer was added?  \n",
    "\n",
    "Let's try it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## (1) Parms\n",
    "numHiddenNodes = 600 \n",
    "w_1 = theano.shared(np.asarray((np.random.randn(*(numFeatures, numHiddenNodes))*.01)))\n",
    "w_2 = theano.shared(np.asarray((np.random.randn(*(numHiddenNodes, numHiddenNodes))*.01)))\n",
    "w_3 = theano.shared(np.asarray((np.random.randn(*(numHiddenNodes, numClasses))*.01)))\n",
    "params = [w_1, w_2, w_3]\n",
    "\n",
    "## (2) Model\n",
    "X = T.matrix()\n",
    "Y = T.matrix()\n",
    "def model(X, w_1, w_2, w_3):\n",
    "    return T.nnet.softmax(T.dot(T.nnet.sigmoid(T.dot(T.nnet.sigmoid(T.dot(X, w_1)), w_2)), w_3))\n",
    "y_hat = model(X, w_1, w_2, w_3)\n",
    "\n",
    "\n",
    "## (3) Cost...same as logistic regression\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(y_hat, Y))\n",
    "\n",
    "\n",
    "## (4) Minimization.  Update rule changes to backpropagation.\n",
    "alpha = 0.01\n",
    "def backprop(cost, w):\n",
    "    grads = T.grad(cost=cost, wrt=w)\n",
    "    updates = []\n",
    "    for w1, grad in zip(w, grads):\n",
    "        updates.append([w1, w1 - grad * alpha])\n",
    "    return updates\n",
    "update = backprop(cost, params)\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True)\n",
    "y_pred = T.argmax(y_hat, axis=1)\n",
    "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
    "\n",
    "miniBatchSize = 1 \n",
    "def gradientDescentStochastic(epochs):\n",
    "    trainTime = 0.0\n",
    "    predictTime = 0.0\n",
    "    start_time = time.time()\n",
    "    for i in range(epochs):\n",
    "        for start, end in zip(range(0, len(train_data), miniBatchSize), range(miniBatchSize, len(train_data), miniBatchSize)):\n",
    "            cost = train(train_data[start:end], train_labels_b[start:end])\n",
    "        trainTime =  trainTime + (time.time() - start_time)\n",
    "        print ('%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(test_labels_b, axis=1) == predict(test_data))))\n",
    "    print ('train time = %.2f' %(trainTime))\n",
    "\n",
    "gradientDescentStochastic(10)\n",
    "\n",
    "start_time = time.time()\n",
    "predict(test_data)   \n",
    "print ('predict time = %.2f' %(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Revisted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a recent idea around activation closely associated with deep learning.  In 2010, in a paper published at NIPS (https://www.utc.fr/~bordesan/dokuwiki/_media/en/glorot10nipsworkshop.pdf), Yoshua Bengio showed that rectifier activation works better empirically than sigmoid activation when used in the hidden layers.  \n",
    "\n",
    "The rectifier activation is simple: f(x)=max(0,x).  Intuitively, the difference is that as a sigmoid activated node approaches 1 it stops learning even if error continues to be propagated to it, whereas the rectifier activated node continue to learn (at least in the positive direction).  It is not completely understood (per Yoshua Bengio) why this helps, but there are some theories being explored including as related to the benefits of sparse representations in networks. (http://www.iro.umontreal.ca/~bengioy/talks/KDD2014-tutorial.pdf).  Rectifiers also speed up training.\n",
    "\n",
    "Although the paper was published in 2010, the technique didn't gain widespread adoption until 2012 when members of Hinton's group spread the word, including with this Kaggle entry: http://blog.kaggle.com/2012/11/01/deep-learning-how-i-did-it-merck-1st-place-interview/\n",
    "\n",
    "Let's change the activation in our 2 layer network to rectifier and see what happens..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## (1) Parms\n",
    "numHiddenNodes = 600 \n",
    "w_1 = theano.shared(np.asarray((np.random.randn(*(numFeatures, numHiddenNodes))*.01)))\n",
    "w_2 = theano.shared(np.asarray((np.random.randn(*(numHiddenNodes, numClasses))*.01)))\n",
    "params = [w_1, w_2]\n",
    "\n",
    "\n",
    "## (2) Model\n",
    "X = T.matrix()\n",
    "Y = T.matrix()\n",
    "\n",
    "def model(X, w_1, w_2):\n",
    "    return T.nnet.softmax(T.dot(T.maximum(T.dot(X, w_1), 0.), w_2))\n",
    "y_hat = model(X, w_1, w_2)\n",
    "\n",
    "\n",
    "## (3) Cost...same as logistic regression\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(y_hat, Y))\n",
    "\n",
    "\n",
    "## (4) Minimization.  Update rule changes to backpropagation.\n",
    "alpha = 0.01\n",
    "def backprop(cost, w):\n",
    "    grads = T.grad(cost=cost, wrt=w)\n",
    "    updates = []\n",
    "    for w1, grad in zip(w, grads):\n",
    "        updates.append([w1, w1 - grad * alpha])\n",
    "    return updates\n",
    "update = backprop(cost, params)\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True)\n",
    "y_pred = T.argmax(y_hat, axis=1)\n",
    "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
    "\n",
    "miniBatchSize = 1 \n",
    "def gradientDescentStochastic(epochs):\n",
    "    trainTime = 0.0\n",
    "    predictTime = 0.0\n",
    "    start_time = time.time()\n",
    "    for i in range(epochs):\n",
    "        for start, end in zip(range(0, len(train_data), miniBatchSize), range(miniBatchSize, len(train_data), miniBatchSize)):\n",
    "            cost = train(train_data[start:end], train_labels_b[start:end])\n",
    "        trainTime =  trainTime + (time.time() - start_time)\n",
    "        print ('%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(test_labels_b, axis=1) == predict(test_data))))\n",
    "    print ('train time = %.2f' %(trainTime))\n",
    "\n",
    "gradientDescentStochastic(50)\n",
    "\n",
    "start_time = time.time()\n",
    "predict(test_data)   \n",
    "print ('predict time = %.2f' %(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maxout Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So rectifier activation worked great!  \n",
    "\n",
    "Exercise: try another type of activation called Maxout (or Max Pooling) activiation.  Maxout activation just selects the max input as the output.  Maxout is a type of pooling, a technique which performs particularly well for vision problems. For more background see: http://jmlr.org/proceedings/papers/v28/goodfellow13.pdf and http://www.quora.com/What-is-impact-of-different-pooling-methods-in-convolutional-neural-networks  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously when working with the MNIST data we saw a benefit in generalization from adding noise to the training data.  Let's try that again here, however this time with a trick for adding noise called 'Dropouts'.  The idea with dropouts is that instead of (or in addition to) adding noise to our inputs, we add noise by having each node return 0 with a certain probability during training.  This trick both improves generalization in large networks and speeds up training.\n",
    "\n",
    "Hinton introduced the idea in 2012 and gave an explanation of why it's similar to bagging (http://arxiv.org/pdf/1207.0580v1.pdf)\n",
    "\n",
    "Let's give it a try..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "## (1) Parms\n",
    "numHiddenNodes = 600 \n",
    "w_1 = theano.shared(np.asarray((np.random.randn(*(numFeatures, numHiddenNodes))*.01)))\n",
    "w_2 = theano.shared(np.asarray((np.random.randn(*(numHiddenNodes, numClasses))*.01)))\n",
    "params = [w_1, w_2]\n",
    "\n",
    "\n",
    "## (2) Model\n",
    "X = T.matrix()\n",
    "Y = T.matrix()\n",
    "srng = RandomStreams()\n",
    "def dropout(X, p=0.):\n",
    "    if p > 0:\n",
    "        X *= srng.binomial(X.shape, p=1 - p)\n",
    "        X /= 1 - p\n",
    "    return X\n",
    "\n",
    "def model(X, w_1, w_2, p_1, p_2):\n",
    "    return T.nnet.softmax(T.dot(dropout(T.maximum(T.dot(dropout(X, p_1), w_1),0.), p_2), w_2))\n",
    "\n",
    "y_hat_train = model(X, w_1, w_2, 0.2, 0.5)\n",
    "y_hat_predict = model(X, w_1, w_2, 0., 0.)\n",
    "\n",
    "## (3) Cost...same as logistic regression\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(y_hat_train, Y))\n",
    "\n",
    "## (4) Minimization.  Update rule changes to backpropagation.\n",
    "alpha = 0.01\n",
    "def backprop(cost, w):\n",
    "    grads = T.grad(cost=cost, wrt=w)\n",
    "    updates = []\n",
    "    for w1, grad in zip(w, grads):\n",
    "        updates.append([w1, w1 - grad * alpha])\n",
    "    return updates\n",
    "update = backprop(cost, params)\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True)\n",
    "y_pred = T.argmax(y_hat_predict, axis=1)\n",
    "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
    "\n",
    "\n",
    "miniBatchSize = 1\n",
    "def gradientDescentStochastic(epochs):\n",
    "    trainTime = 0.0\n",
    "    predictTime = 0.0\n",
    "    start_time = time.time()\n",
    "    for i in range(epochs):\n",
    "        for start, end in zip(range(0, len(train_data), miniBatchSize), range(miniBatchSize, len(train_data), miniBatchSize)):\n",
    "            cost = train(train_data[start:end], train_labels_b[start:end])\n",
    "        trainTime =  trainTime + (time.time() - start_time)\n",
    "        print ('%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(test_labels_b, axis=1) == predict(test_data))))\n",
    "    print ('train time = %.2f' %(trainTime))\n",
    "\n",
    "gradientDescentStochastic(50)\n",
    "\n",
    "start_time = time.time()\n",
    "predict(test_data)   \n",
    "print ('predict time = %.2f' %(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try once again adding a second hidden layer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "## (1) Parmeters\n",
    "numHiddenNodes = 600 \n",
    "w_1 = theano.shared(np.asarray((np.random.randn(*(numFeatures, numHiddenNodes))*.01)))\n",
    "w_2 = theano.shared(np.asarray((np.random.randn(*(numHiddenNodes, numHiddenNodes))*.01)))\n",
    "w_3 = theano.shared(np.asarray((np.random.randn(*(numHiddenNodes, numClasses))*.01)))\n",
    "params = [w_1, w_2, w_3]\n",
    "\n",
    "\n",
    "## (2) Model\n",
    "X = T.matrix()\n",
    "Y = T.matrix()\n",
    "\n",
    "srng = RandomStreams()\n",
    "def dropout(X, p=0.):\n",
    "    if p > 0:\n",
    "        X *= srng.binomial(X.shape, p=1 - p)\n",
    "        X /= 1 - p\n",
    "    return X\n",
    "\n",
    "def model(X, w_1, w_2, w_3, p_1, p_2, p_3):\n",
    "    return T.nnet.softmax(T.dot(dropout(T.maximum(T.dot(dropout(T.maximum(T.dot(dropout(X, p_1), w_1),0.), p_2), w_2),0.), p_3), w_3))\n",
    "\n",
    "y_hat_train = model(X, w_1, w_2, w_3, 0.2, 0.5,0.5)\n",
    "y_hat_predict = model(X, w_1, w_2, w_3, 0., 0.,0.)\n",
    "\n",
    "## (3) Cost...same as logistic regression\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(y_hat_train, Y))\n",
    "\n",
    "\n",
    "## (4) Minimization.  Update rule changes to backpropagation.\n",
    "alpha = 0.01\n",
    "def backprop(cost, w):\n",
    "    grads = T.grad(cost=cost, wrt=w)\n",
    "    updates = []\n",
    "    for w1, grad in zip(w, grads):\n",
    "        updates.append([w1, w1 - grad * alpha])\n",
    "    return updates\n",
    "update = backprop(cost, params)\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True)\n",
    "y_pred = T.argmax(y_hat_predict, axis=1)\n",
    "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
    "\n",
    "miniBatchSize = 1\n",
    "def gradientDescentStochastic(epochs):\n",
    "    trainTime = 0.0\n",
    "    predictTime = 0.0\n",
    "    start_time = time.time()\n",
    "    for i in range(epochs):\n",
    "        for start, end in zip(range(0, len(train_data), miniBatchSize), range(miniBatchSize, len(train_data), miniBatchSize)):\n",
    "            cost = train(train_data[start:end], train_labels_b[start:end])\n",
    "        trainTime =  trainTime + (time.time() - start_time)\n",
    "        print ('%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(test_labels_b, axis=1) == predict(test_data))))\n",
    "    print ('train time = %.2f' %(trainTime))\n",
    "\n",
    "gradientDescentStochastic(10)\n",
    "\n",
    "start_time = time.time()\n",
    "predict(test_data)   \n",
    "print ('predict time = %.2f' %(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3: Convolutional Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, when the phrase 'deep learning' is used to describe a system, it is often a convolution neural network (or convonet).  The convonet architechture was largely developed in the late 90's at Bell Labs, but only very recently popularized.  It was developed for image recognition, and is described and implemented with 2d representations in mind.\n",
    "\n",
    "Geoffrey Hinton has an excellent two-part lecture on the topic:\n",
    "\n",
    "https://www.youtube.com/watch?v=6oD3t6u5EPs\n",
    "\n",
    "https://www.youtube.com/watch?v=fueIAeAsGzA\n",
    "\n",
    "Also, this code was partly taken from these tutorials, which are worth referring back to:\n",
    "\n",
    "http://deeplearning.net/tutorial/lenet.html\n",
    "\n",
    "http://ufldl.stanford.edu/tutorial/supervised/FeatureExtractionUsingConvolution/\n",
    "\n",
    "https://www.youtube.com/watch?v=S75EdAcXHKk\n",
    "\n",
    "http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from theano.tensor.nnet.conv import conv2d\n",
    "from theano.tensor.signal.downsample import max_pool_2d\n",
    "\n",
    "## (1) Parameters\n",
    "numHiddenNodes = 600 \n",
    "patchWidth = 3\n",
    "patchHeight = 3\n",
    "featureMapsLayer1 = 32\n",
    "featureMapsLayer2 = 64\n",
    "featureMapsLayer3 = 128\n",
    "\n",
    "# For convonets, we will work in 2d rather than 1d.  The MNIST images are 28x28 in 2d.\n",
    "imageWidth = 28\n",
    "train_data = train_data.reshape(-1, 1, imageWidth, imageWidth)\n",
    "test_data = test_data.reshape(-1, 1, imageWidth, imageWidth)\n",
    "\n",
    "# Convolution layers.  \n",
    "w_1 = theano.shared(np.asarray((np.random.randn(*(featureMapsLayer1, 1, patchWidth, patchHeight))*.01)))\n",
    "w_2 = theano.shared(np.asarray((np.random.randn(*(featureMapsLayer2, featureMapsLayer1, patchWidth, patchHeight))*.01)))\n",
    "w_3 = theano.shared(np.asarray((np.random.randn(*(featureMapsLayer3, featureMapsLayer2, patchWidth, patchHeight))*.01)))\n",
    "\n",
    "# Fully connected NN. \n",
    "w_4 = theano.shared(np.asarray((np.random.randn(*(featureMapsLayer3 * 3 * 3, numHiddenNodes))*.01)))\n",
    "w_5 = theano.shared(np.asarray((np.random.randn(*(numHiddenNodes, numClasses))*.01)))\n",
    "params = [w_1, w_2, w_3, w_4, w_5]\n",
    "\n",
    "## (2) Model\n",
    "X = T.tensor4() # conv2d works with tensor4 type\n",
    "Y = T.matrix()\n",
    "\n",
    "srng = RandomStreams()\n",
    "def dropout(X, p=0.):\n",
    "    if p > 0:\n",
    "        X *= srng.binomial(X.shape, p=1 - p)\n",
    "        X /= 1 - p\n",
    "    return X\n",
    "\n",
    "# Theano provides built-in support for add convolutional layers\n",
    "def model(X, w_1, w_2, w_3, w_4, w_5, p_1, p_2):\n",
    "    l1 = dropout(max_pool_2d(T.maximum(conv2d(X, w_1, border_mode='full'),0.), (2, 2)), p_1)\n",
    "    l2 = dropout(max_pool_2d(T.maximum(conv2d(l1, w_2), 0.), (2, 2)), p_1)\n",
    "    l3 = dropout(T.flatten(max_pool_2d(T.maximum(conv2d(l2, w_3), 0.), (2, 2)), outdim=2), p_1) # flatten to switch back to 1d layers\n",
    "    l4 = dropout(T.maximum(T.dot(l3, w_4), 0.), p_2)\n",
    "    return T.nnet.softmax(T.dot(l4, w_5))\n",
    "\n",
    "y_hat_train = model(X, w_1, w_2, w_3, w_4, w_5, 0.2, 0.5)\n",
    "y_hat_predict = model(X, w_1, w_2, w_3, w_4, w_5, 0., 0.)\n",
    "y_x = T.argmax(y_hat, axis=1)\n",
    "\n",
    "## (3) Cost\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(y_hat_train, Y))\n",
    "\n",
    "## (4) Minimization.  \n",
    "def backprop(cost, w, alpha=0.001, rho=0.9, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=w)\n",
    "    updates = []\n",
    "    for w1, grad in zip(w, grads):\n",
    "        \n",
    "        # adding gradient scaling\n",
    "        acc = theano.shared(w1.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * grad ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        grad = grad / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        \n",
    "        updates.append((w1, w1 - grad * alpha))\n",
    "    return updates\n",
    "\n",
    "update = backprop(cost, params)\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True)\n",
    "y_pred = T.argmax(y_hat_predict, axis=1)\n",
    "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
    "\n",
    "miniBatchSize = 1\n",
    "def gradientDescentStochastic(epochs):\n",
    "    trainTime = 0.0\n",
    "    predictTime = 0.0\n",
    "    start_time = time.time()\n",
    "    for i in range(epochs):\n",
    "        for start, end in zip(range(0, len(train_data), miniBatchSize), range(miniBatchSize, len(train_data), miniBatchSize)):\n",
    "            cost = train(train_data[start:end], train_labels_b[start:end])\n",
    "        trainTime =  trainTime + (time.time() - start_time)\n",
    "        print ('%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(test_labels_b, axis=1) == predict(test_data))))\n",
    "    print ('train time = %.2f' %(trainTime))\n",
    "\n",
    "gradientDescentStochastic(10)\n",
    "\n",
    "start_time = time.time()\n",
    "predict(test_data)   \n",
    "print ('predict time = %.2f' %(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional topic: Brain inspiration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architechture of the convonet was inspired by the visual cortext in the human brain.  If you are interested in learning more, check out: http://www-psych.stanford.edu/~ashas/Cognition%20Textbook/chapter2.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional topic: Self-Driving Vehicles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional networks are starting to be used more and more for self-driving vehicles.  This past year, NVIDEA started produced a chipset for self-driving vehicles that analyzes the video of up to 14 cameras using convonets for \n",
    "\n",
    "CES 2015 parts 6,7,9\n",
    "\n",
    "https://www.youtube.com/watch?v=-vKGkxeflGw\n",
    "\n",
    "https://www.youtube.com/watch?v=zsVsUvx8ieo\n",
    "\n",
    "https://www.youtube.com/watch?v=RvQVyGOynFY\n",
    "\n",
    "GTC 2015 parts 4,5,7,9\n",
    "\n",
    "https://www.youtube.com/watch?v=pqvdZ2jp1NA\n",
    "\n",
    "https://www.youtube.com/watch?v=GGxdP_JWhwI\n",
    "\n",
    "https://www.youtube.com/watch?v=Tb7ZYSTYHbw\n",
    "\n",
    "https://www.youtube.com/watch?v=TDm6Snkle70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
